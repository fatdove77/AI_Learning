# Transformers

1. **Model（模型）**：
   - 模型是一个经过预训练或者微调用于特定任务的神经网络架构。在自然语言处理（NLP）中，这些模型通常用于各种任务，如文本分类、命名实体识别、文本生成等。例如，BERT、GPT、RoBERTa等都是常见的模型架构。
   - 在Hugging Face中，可以使用`AutoModel`类加载模型。例如，`AutoModelForSequenceClassification`用于加载用于序列分类任务的模型，`AutoModelForQuestionAnswering`用于加载用于问答任务的模型等等。
2. **Tokenizer（分词器）**：
   - 分词器是用于将输入文本转换为模型可以理解的格式的工具。它将输入文本分割成单词或子词，并将其转换为模型需要的输入格式（例如，词嵌入或者token ID序列）。
   - 在Hugging Face中，可以使用`AutoTokenizer`类加载分词器。分词器的选择通常与模型的选择密切相关，因为它们需要匹配以便正确处理模型的输入。

这两个概念都是适用于所有的transformers模型的。不过，具体的模型和分词器会有不同的实现，以满足不同的任务需求。

对于没有任何经验的人来说，在使用Hugging Face的transformers库时，您应该了解以下概念：

- **预训练模型**：这些模型是在大规模的文本数据上进行了预训练的，以学习通用的语言表示。在进行特定任务时，可以微调这些模型以适应特定的领域或任务。
- **微调**：微调是指在预训练模型的基础上，使用特定任务的数据进一步训练模型。这有助于模型更好地适应特定任务的要求。
- **任务**：在NLP中，任务可以是各种文本处理任务，如文本分类、命名实体识别、语言生成等。不同的任务可能需要不同的模型架构和训练技术。
- **输入表示**：这指的是将文本转换为模型可以理解的格式。这可能包括将文本分词、转换为词嵌入或token ID序列等步骤。